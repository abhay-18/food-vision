{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15052ed2-29a5-4685-a7bd-1dd5f135cafc",
      "metadata": {
        "id": "15052ed2-29a5-4685-a7bd-1dd5f135cafc"
      },
      "source": [
        "# Food Vision\n",
        "\n",
        "Food Vision is a model that will classify the food images into 101 classes.\n",
        "The training data consists of 75,750 training images and testing data consists of 25,250 images. Food101 dataset is used.\n",
        "\n",
        "\n",
        "The algorithm uses two methods to significantly improve the speed of model training:\n",
        "1. Prefetching\n",
        "2. Mixed precision training\n",
        "\n",
        "\n",
        "* Using TensorFlow Datasets to download and explore data\n",
        "* Creating preprocessing function for our data\n",
        "* Batching & preparing datasets for modelling (**making our datasets run fast**)\n",
        "* Creating modelling callbacks\n",
        "* Setting up **mixed precision training**\n",
        "* Building a feature extraction model (see [transfer learning part 1: feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb))\n",
        "* Fine-tuning the feature extraction model (see [transfer learning part 2: fine-tuning](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb))\n",
        "* Viewing training results on TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b0a6fc-7cbb-43f3-9de5-abe6bd8f53a9",
      "metadata": {
        "id": "d2b0a6fc-7cbb-43f3-9de5-abe6bd8f53a9"
      },
      "source": [
        "## Check GPU\n",
        "\n",
        "For this notebook, we're going to be doing something different.\n",
        "\n",
        "We're going to be using mixed precision training.\n",
        "\n",
        "Mixed precision training was introduced in [TensorFlow 2.4.0](https://blog.tensorflow.org/2020/12/whats-new-in-tensorflow-24.html) (a very new feature at the time of writing).\n",
        "\n",
        "What does **mixed precision training** do?\n",
        "\n",
        "Mixed precision training uses a combination of single precision (float32) and half-preicison (float16) data types to speed up model training (up 3x on modern GPUs).\n",
        "\n",
        "We'll talk about this more later on but in the meantime you can read the [TensorFlow documentation on mixed precision](https://www.tensorflow.org/guide/mixed_precision) for more details.\n",
        "\n",
        "For now, before we can move forward if we want to use mixed precision training, we need to make sure the GPU powering our Google Colab instance (if you're using Google Colab) is compataible.\n",
        "\n",
        "For mixed precision training to work, **you need access to a GPU with a compute compability score of 7.0+**.\n",
        "\n",
        "Google Colab offers several kinds of GPU.\n",
        "\n",
        "However, some of them **aren't compatiable with mixed precision training.**\n",
        "\n",
        "Therefore to make sure you have access to mixed precision training in Google Colab, you can check your GPU compute capability score on [Nvidia's developer website](https://developer.nvidia.com/cuda-gpus#compute).\n",
        "\n",
        "As of May 2023, the GPUs available on Google Colab which allow mixed precision training are:\n",
        "* NVIDIA A100 (available with Google Colab Pro)\n",
        "* NVIDIA Tesla T4\n",
        "\n",
        "> ðŸ”‘ **Note:** You can run the cell below to check your GPU name and then compare it to [list of GPUs on NVIDIA's developer page](https://developer.nvidia.com/cuda-gpus#compute) to see if it's capable of using mixed precision training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5daf90cd-d153-4193-8e4c-aab705679323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5daf90cd-d153-4193-8e4c-aab705679323",
        "outputId": "e627867e-fd70-49a2-d451-dcfa65225465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Get GPU name\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c25cc91-11e8-442b-9d9f-37ac32e97dbb",
      "metadata": {
        "id": "0c25cc91-11e8-442b-9d9f-37ac32e97dbb"
      },
      "source": [
        "Since mixed precision training was introduced in TensorFlow 2.4.0, make sure you've got at least TensorFlow 2.4.0+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2bcbb711-ed02-4616-830b-b13212215eaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bcbb711-ed02-4616-830b-b13212215eaf",
        "outputId": "404bd387-5810-4851-d2ea-0d477498971f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.11.0\n",
            "Notebook last run (end-to-end): 2024-02-24 21:14:57.193831\n"
          ]
        }
      ],
      "source": [
        "# Note: As of May 2023, there have been some issues with TensorFlow versions 2.9-2.12\n",
        "# with the following code.\n",
        "# However, these seemed to have been fixed in version 2.13+.\n",
        "# TensorFlow version 2.13 is available in tf-nightly as of May 2023 (will be default in Google Colab soon).\n",
        "# Therefore, to prevent errors we'll install tf-nightly first.\n",
        "# See more here: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550\n",
        "\n",
        "# Install tf-nightly (required until 2.13.0+ is the default in Google Colab)\n",
        "# !pip install -U -q tf-nightly\n",
        "\n",
        "# Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better)\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Add timestamp\n",
        "import datetime\n",
        "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80666c6-53e0-433c-996e-9094c6a4cc29",
      "metadata": {
        "id": "d80666c6-53e0-433c-996e-9094c6a4cc29"
      },
      "source": [
        "## Get helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "v4xP7_ja67pX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4xP7_ja67pX",
        "outputId": "0f699e1a-4ab0-4f91-ce83-1431b8d653c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 'helper_functions.py' already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Get helper functions file\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02363842-bbc3-482d-86c2-90c7d093f7fa",
      "metadata": {
        "id": "02363842-bbc3-482d-86c2-90c7d093f7fa"
      },
      "outputs": [],
      "source": [
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e737e5-e729-4492-8fa2-7f64f376164f",
      "metadata": {
        "id": "87e737e5-e729-4492-8fa2-7f64f376164f"
      },
      "source": [
        "## Use TensorFlow Datasets to Download Data\n",
        "\n",
        "In previous notebooks, we've downloaded our food images (from the [Food101 dataset](https://www.kaggle.com/dansbecker/food-101/home)) from Google Storage.\n",
        "\n",
        "And this is a typical workflow you'd use if you're working on your own datasets.\n",
        "\n",
        "However, there's another way to get datasets ready to use with TensorFlow.\n",
        "\n",
        "For many of the most popular datasets in the machine learning world (often referred to and used as benchmarks), you can access them through [TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/overview).\n",
        "\n",
        "What is **TensorFlow Datasets**?\n",
        "\n",
        "A place for prepared and ready-to-use machine learning datasets.\n",
        "\n",
        "Why use TensorFlow Datasets?\n",
        "\n",
        "* Load data already in Tensors\n",
        "* Practice on well established datasets\n",
        "* Experiment with differet data loading techniques (like we're going to use in this notebook)\n",
        "* Experiment with new TensorFlow features quickly (such as mixed precision training)\n",
        "\n",
        "Why *not* use TensorFlow Datasets?\n",
        "\n",
        "* The datasets are static (they don't change, like your real-world datasets would)\n",
        "* Might not be suited for your particular problem (but great for experimenting)\n",
        "\n",
        "To begin using TensorFlow Datasets we can import it under the alias `tfds`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1101abf1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_datasets\n",
            "  Obtaining dependency information for tensorflow_datasets from https://files.pythonhosted.org/packages/fe/18/4865973f5469cfe33bbe1cfc2f1918335eb44f4cc3d316c1bce22c1af2bc/tensorflow_datasets-4.9.4-py3-none-any.whl.metadata\n",
            "  Downloading tensorflow_datasets-4.9.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting absl-py (from tensorflow_datasets)\n",
            "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
            "Collecting dm-tree (from tensorflow_datasets)\n",
            "  Obtaining dependency information for dm-tree from https://files.pythonhosted.org/packages/e4/c1/522041457444b67125ac9527208bb3148f63d7dce0a86ffa589ec763a10e/dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
            "Collecting etils[enp,epath,etree]>=0.9.0 (from tensorflow_datasets)\n",
            "  Obtaining dependency information for etils[enp,epath,etree]>=0.9.0 from https://files.pythonhosted.org/packages/37/10/dd5b124f037a636783e416a2fe839edd7ec63c0dce7ce4f3c1da029aeb80/etils-1.7.0-py3-none-any.whl.metadata\n",
            "  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from tensorflow_datasets) (1.26.2)\n",
            "Collecting promise (from tensorflow_datasets)\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: protobuf>=3.20 in c:\\python311\\lib\\site-packages (from tensorflow_datasets) (4.21.12)\n",
            "Requirement already satisfied: psutil in c:\\users\\abhay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (5.9.8)\n",
            "Collecting requests>=2.19.0 (from tensorflow_datasets)\n",
            "  Obtaining dependency information for requests>=2.19.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
            "  Obtaining dependency information for tensorflow-metadata from https://files.pythonhosted.org/packages/41/23/3705c7139886c079ef4c0e3be56a5a1fb90e9ee413a4b7caaee0ee0ea6fe/tensorflow_metadata-1.14.0-py3-none-any.whl.metadata\n",
            "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting termcolor (from tensorflow_datasets)\n",
            "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting toml (from tensorflow_datasets)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting tqdm (from tensorflow_datasets)\n",
            "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting wrapt (from tensorflow_datasets)\n",
            "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/cf/c3/0084351951d9579ae83a3d9e38c140371e4c6b038136909235079f2e6e78/wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl.metadata\n",
            "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: typing_extensions in c:\\python311\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.8.0)\n",
            "Collecting zipp (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for zipp from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/57/ec/80c8d48ac8b1741d5b963797b7c0c869335619e13d4744ca2f67fc11c6fc/charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.19.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
            "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a2/73/a68704750a7679d0b6d3ad7aa8d4da8e14e151ae82e6fee774e6e0d05ec8/urllib3-2.2.1-py3-none-any.whl.metadata\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.19.0->tensorflow_datasets)\n",
            "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/ba/06/a07f096c664aeb9f01624f858c3add0a4e913d6c96257acb4fce61e7de14/certifi-2024.2.2-py3-none-any.whl.metadata\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
            "Requirement already satisfied: six in c:\\python311\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
            "Collecting absl-py (from tensorflow_datasets)\n",
            "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/dd/87/de5c32fa1b1c6c3305d576e299801d8655c175ca9557019906247b994331/absl_py-1.4.0-py3-none-any.whl.metadata\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
            "  Obtaining dependency information for googleapis-common-protos<2,>=1.52.0 from https://files.pythonhosted.org/packages/f0/43/c9d8f75ddf08e2a0a27db243c13a700c3cc7ec615b545b697cf6f715ad92/googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
            "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/8d/14/619e24a4c70df2901e1f4dbc50a6291eb63a759172558df326347dce1f0d/protobuf-3.20.3-py2.py3-none-any.whl.metadata\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading tensorflow_datasets-4.9.4-py3-none-any.whl (5.1 MB)\n",
            "   ---------------------------------------- 5.1/5.1 MB 432.9 kB/s eta 0:00:00\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "   ---------------------------------------- 62.6/62.6 kB 139.6 kB/s eta 0:00:00\n",
            "Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
            "   -------------------------------------- 101.3/101.3 kB 224.4 kB/s eta 0:00:00\n",
            "Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "   -------------------------------------- 126.5/126.5 kB 392.9 kB/s eta 0:00:00\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "   -------------------------------------- 162.1/162.1 kB 388.5 kB/s eta 0:00:00\n",
            "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 78.3/78.3 kB 145.1 kB/s eta 0:00:00\n",
            "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
            "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "   -------------------------------------- 163.8/163.8 kB 298.1 kB/s eta 0:00:00\n",
            "Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
            "   ---------------------------------------- 99.9/99.9 kB 338.2 kB/s eta 0:00:00\n",
            "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
            "   -------------------------------------- 228.7/228.7 kB 423.3 kB/s eta 0:00:00\n",
            "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "   ---------------------------------------- 61.6/61.6 kB 88.8 kB/s eta 0:00:00\n",
            "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "   -------------------------------------- 121.1/121.1 kB 202.6 kB/s eta 0:00:00\n",
            "Downloading etils-1.7.0-py3-none-any.whl (152 kB)\n",
            "   -------------------------------------- 152.4/152.4 kB 478.7 kB/s eta 0:00:00\n",
            "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "   -------------------------------------- 170.9/170.9 kB 209.8 kB/s eta 0:00:00\n",
            "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Building wheels for collected packages: promise\n",
            "  Building wheel for promise (pyproject.toml): started\n",
            "  Building wheel for promise (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21545 sha256=a73677fa94f6e204274a8b6c2bb5b7677071cc87114d059df6c098dabd7d59e5\n",
            "  Stored in directory: c:\\users\\abhay\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
            "Successfully built promise\n",
            "Installing collected packages: dm-tree, zipp, wrapt, urllib3, tqdm, toml, termcolor, protobuf, promise, importlib_resources, idna, fsspec, etils, charset-normalizer, certifi, absl-py, requests, googleapis-common-protos, tensorflow-metadata, tensorflow_datasets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
            "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe' -> 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe.deleteme'\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef4f160e-55f6-47e0-8c4e-fba7d1dfd91b",
      "metadata": {
        "id": "ef4f160e-55f6-47e0-8c4e-fba7d1dfd91b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get TensorFlow Datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
          ]
        }
      ],
      "source": [
        "# Get TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbb4d29-aeb1-4ed5-928e-334cc3637b13",
      "metadata": {
        "id": "6fbb4d29-aeb1-4ed5-928e-334cc3637b13"
      },
      "source": [
        "To find all of the available datasets in TensorFlow Datasets, you can use the `list_builders()` method.\n",
        "\n",
        "After doing so, we can check to see if the one we're after (`\"food101\"`) is present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1aeb0578-b57f-4993-a834-8246aca0e252",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aeb0578-b57f-4993-a834-8246aca0e252",
        "outputId": "2da1e580-e002-49ea-ddc9-689db016868f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'food101' in TensorFlow Datasets: True\n"
          ]
        }
      ],
      "source": [
        "# Get all available datasets in TFDS\n",
        "datasets_list = tfds.list_builders()\n",
        "\n",
        "# Set our target dataset and see if it exists\n",
        "target_dataset = \"food101\"\n",
        "print(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f9c2ba-5386-4eca-a26e-1fb564287faa",
      "metadata": {
        "id": "f9f9c2ba-5386-4eca-a26e-1fb564287faa"
      },
      "source": [
        "Beautiful! It looks like the dataset we're after is available (note there are plenty more available but we're on Food101).\n",
        "\n",
        "To get access to the Food101 dataset from the TFDS, we can use the [`tfds.load()`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) method.\n",
        "\n",
        "In particular, we'll have to pass it a few parameters to let it know what we're after:\n",
        "* `name` (str) : the target dataset (e.g. `\"food101\"`)\n",
        "* `split` (list, optional) : what splits of the dataset we're after (e.g. `[\"train\", \"validation\"]`)\n",
        "  * the `split` parameter is quite tricky. See [the documentation for more](https://github.com/tensorflow/datasets/blob/master/docs/splits.md).\n",
        "* `shuffle_files` (bool) : whether or not to shuffle the files on download, defaults to `False`\n",
        "* `as_supervised` (bool) : `True` to download data samples in tuple format (`(data, label)`) or `False` for dictionary format\n",
        "* `with_info` (bool) : `True` to download dataset metadata (labels, number of samples, etc)\n",
        "\n",
        "> ðŸ”‘ **Note:** Calling the `tfds.load()` method will start to download a target dataset to disk if the `download=True` parameter is set (default). This dataset could be 100GB+, so make sure you have space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ccd73b2-37df-4402-afc5-49de4dea4c77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "6960acc878f842ea827fcd7a3bba8034",
            "3e5ccd7b921646ce9c98c63650c0b91c",
            "5a8d50941efd48b9abb954658469efc2",
            "53de0d0d65ba48fca265882843128ab2",
            "498125d566fc4b479977d2fcb670686b",
            "1a338dc88995461c82e3088daa647f32",
            "f32318cbbd5444f29ab875a2a63246b1",
            "a4c37537333345c58b911cbd47f65a12",
            "6ddbe04d22b3480fa898aec7b9b0ca17",
            "c0e6d6f2317b49a080f3fb5d8d2b1936",
            "96aa21c821a3440fb2e20a80a34e5101",
            "4eb8b9828f3a49abab40e64cd09af9fb",
            "636f6915f8414efcaade38959ff052ef",
            "30d307e20e19487eb77bfacb9e5db6cb",
            "f42698984d274febbb077cd115cd41ca",
            "cc04768d14c94f7db02e686414425576",
            "fe9583bfed6c498a8a22df36567ed585",
            "379aed682ef94d73adaaea88ca54278a",
            "bf45f4fb71f14761bd497740051c7e24",
            "09d4a35af4e94860bcc45482bfd11c36",
            "a51ec4caffae40b5aa0c6090c57c2050",
            "cb84865f4e4b4b8590ef0a053c5eccce",
            "7bd2692da1194d56acf1a497a0e81afb",
            "ab73b901ce74498a839d328834fdab0c",
            "06eb0ec7e76c4e13a52c6fe64176cbf5",
            "f076394f8e624bbf8a0550be45923395",
            "3070680f2783400790aeff5475a559f5",
            "60259aa3d1d14c94a34fd2592244ce8a",
            "106d43e34bd449a2ba467951ec262b06",
            "2bd0039e3e104a9799ce92f830de6b87",
            "4388427f02b745ac9ceadd70a2bf3f71",
            "83ca4a1705504b8a9c1b9f10973409e9",
            "db683af2888a4ae7a07b4e472b9ea617"
          ]
        },
        "id": "5ccd73b2-37df-4402-afc5-49de4dea4c77",
        "outputId": "4c387c4f-f3cc-4cc0-a2c5-0d49b3b08189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset 4.65 GiB (download: 4.65 GiB, generated: Unknown size, total: 4.65 GiB) to /root/tensorflow_datasets/food101/2.0.0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6960acc878f842ea827fcd7a3bba8034",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eb8b9828f3a49abab40e64cd09af9fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bd2692da1194d56acf1a497a0e81afb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load in the data (takes about 5-6 minutes in Google Colab)\n",
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True,\n",
        "                                             __path__ = 'data') # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51033691-941b-4659-80df-9461689753f0",
      "metadata": {
        "id": "51033691-941b-4659-80df-9461689753f0"
      },
      "source": [
        "Wonderful! After a few minutes of downloading, we've now got access to entire Food101 dataset (in tensor format) ready for modelling.\n",
        "\n",
        "Now let's get a little information from our dataset, starting with the class names.\n",
        "\n",
        "Getting class names from a TensorFlow Datasets dataset requires downloading the \"`dataset_info`\" variable (by using the `as_supervised=True` parameter in the `tfds.load()` method, **note:** this will only work for supervised datasets in TFDS).\n",
        "\n",
        "We can access the class names of a particular dataset using the `dataset_info.features` attribute and accessing `names` attribute of the the `\"label\"` key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6115265e-469b-4fcd-a54b-7fb4e59b90af",
      "metadata": {
        "id": "6115265e-469b-4fcd-a54b-7fb4e59b90af"
      },
      "outputs": [],
      "source": [
        "# Features of Food101 TFDS\n",
        "ds_info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51532ab7-9ffa-4d19-9726-389984f39a04",
      "metadata": {
        "id": "51532ab7-9ffa-4d19-9726-389984f39a04"
      },
      "outputs": [],
      "source": [
        "# Get class names\n",
        "class_names = ds_info.features[\"label\"].names\n",
        "class_names[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "896c2153-9420-432d-a424-704424789ba8",
      "metadata": {
        "id": "896c2153-9420-432d-a424-704424789ba8"
      },
      "source": [
        "### Exploring the Food101 data from TensorFlow Datasets\n",
        "\n",
        "Now we've downloaded the Food101 dataset from TensorFlow Datasets, how about we do what any good data explorer should?\n",
        "\n",
        "In other words, \"visualize, visualize, visualize\".\n",
        "\n",
        "Let's find out a few details about our dataset:\n",
        "* The shape of our input data (image tensors)\n",
        "* The datatype of our input data\n",
        "* What the labels of our input data look like (e.g. one-hot encoded versus label-encoded)\n",
        "* Do the labels match up with the class names?\n",
        "\n",
        "To do, let's take one sample off the training data (using the [`.take()` method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take)) and explore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b89a61-50e3-4058-9dae-bb0cefacca16",
      "metadata": {
        "id": "60b89a61-50e3-4058-9dae-bb0cefacca16"
      },
      "outputs": [],
      "source": [
        "# Take one sample off the training data\n",
        "train_one_sample = train_data.take(1) # samples are in format (image_tensor, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bb1dfd-7c88-49a1-a174-1f6ba973ad61",
      "metadata": {
        "id": "42bb1dfd-7c88-49a1-a174-1f6ba973ad61"
      },
      "source": [
        "Because we used the `as_supervised=True` parameter in our `tfds.load()` method above, data samples come in the tuple format structure `(data, label)` or in our case `(image_tensor, label)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a13d601-0911-4a36-be2c-a59ca6b809a4",
      "metadata": {
        "id": "8a13d601-0911-4a36-be2c-a59ca6b809a4"
      },
      "outputs": [],
      "source": [
        "# What does one sample of our training data look like?\n",
        "train_one_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83d5488-6cd0-4dd2-8d83-ae80eaf06c73",
      "metadata": {
        "id": "a83d5488-6cd0-4dd2-8d83-ae80eaf06c73"
      },
      "source": [
        "Let's loop through our single training sample and get some info from the `image_tensor` and `label`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "123d5880-776f-4002-8eca-5f1fae500b7a",
      "metadata": {
        "id": "123d5880-776f-4002-8eca-5f1fae500b7a"
      },
      "outputs": [],
      "source": [
        "# Output info about our training sample\n",
        "for image, label in train_one_sample:\n",
        "  print(f\"\"\"\n",
        "  Image shape: {image.shape}\n",
        "  Image dtype: {image.dtype}\n",
        "  Target class from Food101 (tensor form): {label}\n",
        "  Class name (str form): {class_names[label.numpy()]}\n",
        "        \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab240eb4-fedf-42d5-aaba-8906d11af3e2",
      "metadata": {
        "id": "ab240eb4-fedf-42d5-aaba-8906d11af3e2"
      },
      "source": [
        "Because we set the `shuffle_files=True` parameter in our `tfds.load()` method above, running the cell above a few times will give a different result each time.\n",
        "\n",
        "Checking these you might notice some of the images have different shapes, for example `(512, 342, 3)` and `(512, 512, 3)` (height, width, color_channels).\n",
        "\n",
        "Let's see what one of the image tensors from TFDS's Food101 dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc3ed60-3003-4239-ad3b-9a77b4e750f0",
      "metadata": {
        "id": "5bc3ed60-3003-4239-ad3b-9a77b4e750f0"
      },
      "outputs": [],
      "source": [
        "# What does an image tensor from TFDS's Food101 look like?\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808e21e5-6951-4eac-bb3c-00fc1ae3fa0f",
      "metadata": {
        "id": "808e21e5-6951-4eac-bb3c-00fc1ae3fa0f"
      },
      "outputs": [],
      "source": [
        "# What are the min and max values?\n",
        "tf.reduce_min(image), tf.reduce_max(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff7dad2-0972-4905-b14c-549609401d98",
      "metadata": {
        "id": "3ff7dad2-0972-4905-b14c-549609401d98"
      },
      "source": [
        "Alright looks like our image tensors have values of between 0 & 255 (standard red, green, blue colour values) and the values are of data type `unit8`.\n",
        "\n",
        "We might have to preprocess these before passing them to a neural network. But we'll handle this later.\n",
        "\n",
        "In the meantime, let's see if we can plot an image sample."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630fb994-b61f-4a03-8831-83fe50ab6ca1",
      "metadata": {
        "id": "630fb994-b61f-4a03-8831-83fe50ab6ca1"
      },
      "source": [
        "### Plot an image from TensorFlow Datasets\n",
        "\n",
        "We've seen our image tensors in tensor format, now let's really adhere to our motto.\n",
        "\n",
        "\"Visualize, visualize, visualize!\"\n",
        "\n",
        "Let's plot one of the image samples using [`matplotlib.pyplot.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) and set the title to target class name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f19d6a3-b2ac-47e9-a670-c08461897d6c",
      "metadata": {
        "id": "3f19d6a3-b2ac-47e9-a670-c08461897d6c"
      },
      "outputs": [],
      "source": [
        "# Plot an image tensor\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)\n",
        "plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fa7e9a-9918-4bc0-8e0f-c87893922157",
      "metadata": {
        "id": "04fa7e9a-9918-4bc0-8e0f-c87893922157"
      },
      "source": [
        "Delicious!\n",
        "\n",
        "Okay, looks like the Food101 data we've got from TFDS is similar to the datasets we've been using in previous notebooks.\n",
        "\n",
        "Now let's preprocess it and get it ready for use with a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1e2538-2340-4394-8700-6a126f1b2faa",
      "metadata": {
        "id": "8e1e2538-2340-4394-8700-6a126f1b2faa"
      },
      "source": [
        "## Create preprocessing functions for our data\n",
        "\n",
        "In previous notebooks, when our images were in folder format we used the method [`tf.keras.utils.image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) to load them in.\n",
        "\n",
        "Doing this meant our data was loaded into a format ready to be used with our models.\n",
        "\n",
        "However, since we've downloaded the data from TensorFlow Datasets, there are a couple of preprocessing steps we have to take before it's ready to model.\n",
        "\n",
        "More specifically, our data is currently:\n",
        "\n",
        "* In `uint8` data type\n",
        "* Comprised of all differnet sized tensors (different sized images)\n",
        "* Not scaled (the pixel values are between 0 & 255)\n",
        "\n",
        "Whereas, models like data to be:\n",
        "\n",
        "* In `float32` data type\n",
        "* Have all of the same size tensors (batches require all tensors have the same shape, e.g. `(224, 224, 3)`)\n",
        "* Scaled (values between 0 & 1), also called normalized\n",
        "\n",
        "To take care of these, we'll create a `preprocess_img()` function which:\n",
        "\n",
        "* Resizes an input image tensor to a specified size using [`tf.image.resize()`](https://www.tensorflow.org/api_docs/python/tf/image/resize)\n",
        "* Converts an input image tensor's current datatype to `tf.float32` using [`tf.cast()`](https://www.tensorflow.org/api_docs/python/tf/cast)\n",
        "\n",
        "> ðŸ”‘ **Note:** Pretrained EfficientNetBX models in [`tf.keras.applications.efficientnet`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet) (what we're going to be using) have rescaling built-in. But for many other model architectures you'll want to rescale your data (e.g. get its values between 0 & 1). This could be incorporated inside your \"`preprocess_img()`\" function (like the one below) or within your model as a [`tf.keras.layers.Rescaling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling) layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b7307e-165a-4509-8e99-ce484bfc39f8",
      "metadata": {
        "id": "51b7307e-165a-4509-8e99-ce484bfc39f8"
      },
      "outputs": [],
      "source": [
        "# Make a function for preprocessing images\n",
        "def preprocess_img(image, label, img_shape=224):\n",
        "    \"\"\"\n",
        "    Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
        "    [img_shape, img_shape, color_channels]\n",
        "\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
        "    return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253aaffa-6722-46cc-9b4e-2b625cca84fc",
      "metadata": {
        "id": "253aaffa-6722-46cc-9b4e-2b625cca84fc"
      },
      "source": [
        "Our `preprocess_img()` function above takes image and label as input (even though it does nothing to the label) because our dataset is currently in the tuple structure `(image, label)`.\n",
        "\n",
        "Let's try our function out on a target image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdb051f-6e67-4189-a69a-44748d4a7bd5",
      "metadata": {
        "id": "9cdb051f-6e67-4189-a69a-44748d4a7bd5"
      },
      "outputs": [],
      "source": [
        "# Preprocess a single sample image and check the outputs\n",
        "preprocessed_img = preprocess_img(image, label)[0]\n",
        "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\n",
        "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f56a9f-6d66-4c7a-9321-5831391a1e3e",
      "metadata": {
        "id": "b2f56a9f-6d66-4c7a-9321-5831391a1e3e"
      },
      "source": [
        "Excellent! Looks like our `preprocess_img()` function is working as expected.\n",
        "\n",
        "The input image gets converted from `uint8` to `float32` and gets reshaped from its current shape to `(224, 224, 3)`.\n",
        "\n",
        "How does it look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6046678b-0e38-40bf-b4c5-e0d4b0d98f97",
      "metadata": {
        "id": "6046678b-0e38-40bf-b4c5-e0d4b0d98f97"
      },
      "outputs": [],
      "source": [
        "# We can still plot our preprocessed image as long as we\n",
        "# divide by 255 (for matplotlib capatibility)\n",
        "plt.imshow(preprocessed_img/255.)\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3324aa01-f28e-4960-b5e5-0e245310f2d9",
      "metadata": {
        "id": "3324aa01-f28e-4960-b5e5-0e245310f2d9"
      },
      "source": [
        "All this food visualization is making me hungry. How about we start preparing to model it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a4982c-dc50-4aae-b823-9ac13c8d95b6",
      "metadata": {
        "id": "24a4982c-dc50-4aae-b823-9ac13c8d95b6"
      },
      "source": [
        "## Batch & prepare datasets\n",
        "\n",
        "Before we can model our data, we have to turn it into batches.\n",
        "\n",
        "Why?\n",
        "\n",
        "Because computing on batches is memory efficient.\n",
        "\n",
        "We turn our data from 101,000 image tensors and labels (train and test combined) into batches of 32 image and label pairs, thus enabling it to fit into the memory of our GPU.\n",
        "\n",
        "To do this in effective way, we're going to be leveraging a number of methods from the [`tf.data` API](https://www.tensorflow.org/api_docs/python/tf/data).\n",
        "\n",
        "> ðŸ“– **Resource:** For loading data in the most performant way possible, see the TensorFlow docuemntation on [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance).\n",
        "\n",
        "Specifically, we're going to be using:\n",
        "\n",
        "* [`map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) - maps a predefined function to a target dataset (e.g. `preprocess_img()` to our image tensors)\n",
        "* [`shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) - randomly shuffles the elements of a target dataset up `buffer_size` (ideally, the `buffer_size` is equal to the size of the dataset, however, this may have implications on memory)\n",
        "* [`batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) - turns elements of a target dataset into batches (size defined by parameter `batch_size`)\n",
        "* [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) - prepares subsequent batches of data whilst other batches of data are being computed on (improves data loading speed but costs memory)\n",
        "* Extra: [`cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) - caches (saves them for later) elements in a target dataset, saving loading time (will only work if your dataset is small enough to fit in memory, standard Colab instances only have 12GB of memory)\n",
        "\n",
        "Things to note:\n",
        "- Can't batch tensors of different shapes (e.g. different image sizes, need to reshape images first, hence our `preprocess_img()` function)\n",
        "- `shuffle()` keeps a buffer of the number you pass it images shuffled, ideally this number would be all of the samples in your training set, however, if your training set is large, this buffer might not fit in memory (a fairly large number like 1000 or 10000 is usually suffice for shuffling)\n",
        "- For methods with the `num_parallel_calls` parameter available (such as `map()`), setting it to`num_parallel_calls=tf.data.AUTOTUNE` will parallelize preprocessing and significantly improve speed\n",
        "- Can't use `cache()` unless your dataset can fit in memory\n",
        "\n",
        "Woah, the above is alot. But once we've coded below, it'll start to make sense.\n",
        "\n",
        "We're going to through things in the following order:\n",
        "\n",
        "```\n",
        "Original dataset (e.g. train_data) -> map() -> shuffle() -> batch() -> prefetch() -> PrefetchDataset\n",
        "```\n",
        "\n",
        "This is like saying,\n",
        "\n",
        "> \"Hey, map this preprocessing function across our training dataset, then shuffle a number of elements before batching them together and make sure you prepare new batches (prefetch) whilst the model is looking through the current batch\".\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-prefetching-from-hands-on-ml.png)\n",
        "\n",
        "*What happens when you use prefetching (faster) versus what happens when you don't use prefetching (slower). **Source:** Page 422 of [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by AurÃ©lien GÃ©ron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15991d7f-b649-4a3b-b443-e61b56ce653c",
      "metadata": {
        "id": "15991d7f-b649-4a3b-b443-e61b56ce653c"
      },
      "outputs": [],
      "source": [
        "# Map preprocessing function to training data (and paralellize)\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map prepreprocessing function to test data\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Turn test data into batches (don't need to shuffle)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f504e9c7-6bf8-4fdb-acd4-c67abffbf091",
      "metadata": {
        "id": "f504e9c7-6bf8-4fdb-acd4-c67abffbf091"
      },
      "source": [
        "And now let's check out what our prepared datasets look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2f3c75-739c-4eb0-8156-a21dea3de937",
      "metadata": {
        "id": "9b2f3c75-739c-4eb0-8156-a21dea3de937"
      },
      "outputs": [],
      "source": [
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5781af7a-7faa-4fc6-b1e9-8b5db0d233b0",
      "metadata": {
        "id": "5781af7a-7faa-4fc6-b1e9-8b5db0d233b0"
      },
      "source": [
        "Excellent! Looks like our data is now in tutples of `(image, label)` with datatypes of `(tf.float32, tf.int64)`, just what our model is after.\n",
        "\n",
        "> ðŸ”‘ **Note:** You can get away without calling the `prefetch()` method on the end of your datasets, however, you'd probably see significantly slower data loading speeds when building a model. So most of your dataset input pipelines should end with a call to [`prefecth()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch).\n",
        "\n",
        "Onward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df30b415-ee22-4694-b7d2-dfe968e7bd10",
      "metadata": {
        "id": "df30b415-ee22-4694-b7d2-dfe968e7bd10"
      },
      "source": [
        "## Create modelling callbacks\n",
        "\n",
        "Since we're going to be training on a large amount of data and training could take a long time, it's a good idea to set up some modelling callbacks so we be sure of things like our model's training logs being tracked and our model being checkpointed (saved) after various training milestones.\n",
        "\n",
        "To do each of these we'll use the following callbacks:\n",
        "* [`tf.keras.callbacks.TensorBoard()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) - allows us to keep track of our model's training history so we can inspect it later (**note:** we've created this callback before have imported it from `helper_functions.py` as `create_tensorboard_callback()`)\n",
        "* [`tf.keras.callbacks.ModelCheckpoint()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) - saves our model's progress at various intervals so we can load it and resuse it later without having to retrain it\n",
        "  * Checkpointing is also helpful so we can start fine-tuning our model at a particular epoch and revert back to a previous state if fine-tuning offers no benefits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f54a576-3892-4043-a500-9a6b95199dce",
      "metadata": {
        "id": "2f54a576-3892-4043-a500-9a6b95199dce"
      },
      "outputs": [],
      "source": [
        "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path = \"model_checkpoints/.weights.h5\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=0) # don't print out whether or not model is being saved"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a64ec4e-8774-4bdc-a5d2-f0a44f85d50e",
      "metadata": {
        "id": "1a64ec4e-8774-4bdc-a5d2-f0a44f85d50e"
      },
      "source": [
        "## Build feature extraction model\n",
        "\n",
        "Callbacks: ready to roll.\n",
        "\n",
        "Mixed precision: turned on.\n",
        "\n",
        "Let's build a model.\n",
        "\n",
        "Because our dataset is quite large, we're going to move towards fine-tuning an existing pretrained model (EfficienetNetB0).\n",
        "\n",
        "But before we get into fine-tuning, let's set up a feature-extraction model.\n",
        "\n",
        "Recall, the typical order for using transfer learning is:\n",
        "\n",
        "1. Build a feature extraction model (replace the top few layers of a pretrained model)\n",
        "2. Train for a few epochs with lower layers frozen\n",
        "3. Fine-tune if necessary with multiple layers unfrozen\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-feature-extraction-then-fine-tune.png)\n",
        "*Before fine-tuning, it's best practice to train a feature extraction model with custom top layers.*\n",
        "\n",
        "To build the feature extraction model (covered in [Transfer Learning in TensorFlow Part 1: Feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb)), we'll:\n",
        "* Use `EfficientNetB0` from [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) pre-trained on ImageNet as our base model\n",
        "  * We'll download this without the top layers using `include_top=False` parameter so we can create our own output layers\n",
        "* Freeze the base model layers so we can use the pre-learned patterns the base model has found on ImageNet\n",
        "* Put together the input, base model, pooling and output layers in a [Functional model](https://keras.io/guides/functional_api/)\n",
        "* Compile the Functional model using the Adam optimizer and [sparse categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) as the loss function (since our labels **aren't** one-hot encoded)\n",
        "* Fit the model for 3 epochs using the TensorBoard and ModelCheckpoint callbacks\n",
        "\n",
        "> ðŸ”‘ **Note:** Since we're using mixed precision training, our model needs a separate output layer with a hard-coded `dtype=float32`, for example, `layers.Activation(\"softmax\", dtype=tf.float32)`. This ensures the outputs of our model are returned back to the float32 data type which is more numerically stable than the float16 datatype (important for loss calculations). See the [\"Building the model\"](https://www.tensorflow.org/guide/mixed_precision#building_the_model) section in the TensorFlow mixed precision guide for more.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-mixed-precision-code-before-and-after.png)\n",
        "*Turning mixed precision on in TensorFlow with 3 lines of code.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7573b3d3-28bd-469b-a1c4-f19e780fce38",
      "metadata": {
        "id": "7573b3d3-28bd-469b-a1c4-f19e780fce38"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False # freeze base model layers\n",
        "\n",
        "# Create Functional model\n",
        "inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "\n",
        "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "# x = layers.Rescaling(1./255)(x)\n",
        "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x = tf.keras.layers.Dense(len(class_names))(x) # want one output neuron per class\n",
        "# Separate activation of output layer so we can output float32 activations\n",
        "outputs = tf.keras.layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79cdfae0-7229-4923-bf55-9529c2d09134",
      "metadata": {
        "id": "79cdfae0-7229-4923-bf55-9529c2d09134"
      },
      "outputs": [],
      "source": [
        "# Check out our model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d186ff-d01d-46d3-a88e-6fc7a849865f",
      "metadata": {
        "id": "a1d186ff-d01d-46d3-a88e-6fc7a849865f"
      },
      "outputs": [],
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
        "    print(layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ebfec1-b7ad-4d38-b42d-b266d09650a2",
      "metadata": {
        "id": "17ebfec1-b7ad-4d38-b42d-b266d09650a2"
      },
      "source": [
        "## Fit the feature extraction model\n",
        "\n",
        "Now that's one good looking model. Let's fit it to our data shall we?\n",
        "\n",
        "Three epochs should be enough for our top layers to adjust their weights enough to our food image data.\n",
        "\n",
        "To save time per epoch, we'll also only validate on 15% of the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b8f418-391f-4232-af76-102eac0b9d82",
      "metadata": {
        "id": "c1b8f418-391f-4232-af76-102eac0b9d82"
      },
      "outputs": [],
      "source": [
        "# Turn off all warnings except for errors\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Fit the model with callbacks\n",
        "history_101_food_classes_feature_extract = model.fit(train_data,\n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\",\n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29280c9a-c828-4cf5-8753-8c1bfdced3c0",
      "metadata": {
        "id": "29280c9a-c828-4cf5-8753-8c1bfdced3c0"
      },
      "source": [
        "Nice, looks like our feature extraction model is performing pretty well. How about we evaluate it on the whole test dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f30fb985-9735-4cc6-8d61-63451fc73bce",
      "metadata": {
        "id": "f30fb985-9735-4cc6-8d61-63451fc73bce"
      },
      "outputs": [],
      "source": [
        "# Evaluate model (unsaved version) on whole test dataset\n",
        "results_feature_extract_model = model.evaluate(test_data)\n",
        "results_feature_extract_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7f263b-5ae0-4d43-a216-97d868003590",
      "metadata": {
        "id": "4a7f263b-5ae0-4d43-a216-97d868003590"
      },
      "source": [
        "And since we used the `ModelCheckpoint` callback, we've got a saved version of our model in the `model_checkpoints` directory.\n",
        "\n",
        "Let's load it in and make sure it performs just as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "693c7376-892d-4525-83ce-8a82ec12df76",
      "metadata": {
        "id": "693c7376-892d-4525-83ce-8a82ec12df76"
      },
      "source": [
        "## Load and evaluate checkpoint weights\n",
        "\n",
        "We can load in and evaluate our model's checkpoints by:\n",
        "\n",
        "1. Recreating a new instance of our model called `created_model` by turning our original model creation code into a function called `create_model()`.\n",
        "2. Compiling our `created_model` with the same loss, optimizer and metrics as the original model (every time you create a new model, you must compile it).\n",
        "3. Calling the `load_weights()` method on our `created_model` and passing it the path to where our checkpointed weights are stored.\n",
        "4. Calling `evaluate()` on `created_model` with loaded weights and saving the results.\n",
        "5. Comparing the `created_model` results to our previous `model` results (these should be the exact same, if not very close).\n",
        "\n",
        "A reminder, checkpoints are helpful for when you perform an experiment such as fine-tuning your model. In the case you fine-tune your feature extraction model and find it doesn't offer any improvements, you can always revert back to the checkpointed version of your model.\n",
        "\n",
        "> **Note:** This section originally used the [`tf.keras.clone_model` method](https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model), however, due to several potential errors with that method, it changed to create a new model (rather than cloning) via a `create_model()` function. See the [discussion on the course GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550) for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f3PBdfWwoX",
      "metadata": {
        "id": "95f3PBdfWwoX"
      },
      "outputs": [],
      "source": [
        "# 1. Create a function to recreate the original model\n",
        "def create_model():\n",
        "  # Create base model\n",
        "  input_shape = (224, 224, 3)\n",
        "  base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n",
        "  base_model.trainable = False # freeze base model layers\n",
        "\n",
        "  # Create Functional model\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "  # x = layers.Rescaling(1./255)(x)\n",
        "  x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "  x = tf.keras.layers.Dense(len(class_names))(x) # want one output neuron per class\n",
        "  # Separate activation of output layer so we can output float32 activations\n",
        "  outputs = tf.keras.layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "# 2. Create and compile a new version of the original model (new weights)\n",
        "created_model = create_model()\n",
        "created_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Load the saved weights\n",
        "created_model.load_weights(checkpoint_path)\n",
        "\n",
        "# 4. Evaluate the model with loaded weights\n",
        "results_created_model_with_loaded_weights = created_model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4132ed85-f9c8-4380-8d26-c1aedec44fe4",
      "metadata": {
        "id": "4132ed85-f9c8-4380-8d26-c1aedec44fe4"
      },
      "source": [
        "Our `created_model` with loaded weight's results should be very close to the feature extraction model's results (if the cell below errors, something went wrong)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65f5b3f-27a8-44d4-a3fa-52f1055f37f2",
      "metadata": {
        "id": "c65f5b3f-27a8-44d4-a3fa-52f1055f37f2"
      },
      "outputs": [],
      "source": [
        "# Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\n",
        "save_dir = \"efficientnetb0_feature_extract_model.keras\"\n",
        "model.save(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006eef4e-b9e6-415c-b69e-1583ffc1d1d0",
      "metadata": {
        "id": "006eef4e-b9e6-415c-b69e-1583ffc1d1d0"
      },
      "source": [
        "And again, we can check whether or not our model saved correctly by loading it in and evaluating it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927b1e9d-ebd9-47c4-b167-83950a2bd58d",
      "metadata": {
        "id": "927b1e9d-ebd9-47c4-b167-83950a2bd58d"
      },
      "outputs": [],
      "source": [
        "# Load model previously saved above\n",
        "loaded_saved_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd05f8f5-3470-40ae-a754-e644e2c8e5e4",
      "metadata": {
        "id": "fd05f8f5-3470-40ae-a754-e644e2c8e5e4"
      },
      "outputs": [],
      "source": [
        "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
        "results_loaded_saved_model = loaded_saved_model.evaluate(test_data)\n",
        "results_loaded_saved_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359b51f5-2f97-42d8-835c-d84cef3c6966",
      "metadata": {
        "id": "359b51f5-2f97-42d8-835c-d84cef3c6966"
      },
      "source": [
        "## Preparing our model's layers for fine-tuning\n",
        "\n",
        "Our feature-extraction model is showing some great promise after three epochs. But since we've got so much data, it's probably worthwhile that we see what results we can get with fine-tuning (fine-tuning usually works best when you've got quite a large amount of data).\n",
        "\n",
        "Remember our goal of beating the [DeepFood paper](https://arxiv.org/pdf/1606.05675.pdf)?\n",
        "\n",
        "They were able to achieve 77.4% top-1 accuracy on Food101 over 2-3 days of training.\n",
        "\n",
        "Do you think fine-tuning will get us there?\n",
        "\n",
        "Let's find out.\n",
        "\n",
        "To start, let's load in our saved model.\n",
        "\n",
        "> ðŸ”‘ **Note:** It's worth remembering a traditional workflow for fine-tuning is to freeze a pre-trained base model and then train only the output layers for a few iterations so their weights can be updated inline with your custom data (feature extraction). And then unfreeze a number or all of the layers in the base model and continue training until the model stops improving.\n",
        "\n",
        "Like all good cooking shows, I've saved a model I prepared earlier (the feature extraction model from above) to Google Storage.\n",
        "\n",
        "We can download it to make sure we're using the same model going forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d135faa-3ad7-4502-80b8-ea0f931f0c5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d135faa-3ad7-4502-80b8-ea0f931f0c5c",
        "outputId": "7da5afc1-943e-48e5-a6f1-2d252a0fead0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-19 02:28:24--  https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 142.251.10.128, 142.251.12.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16976857 (16M) [application/zip]\n",
            "Saving to: â€˜07_efficientnetb0_feature_extract_model_mixed_precision.zipâ€™\n",
            "\n",
            "07_efficientnetb0_f 100%[===================>]  16.19M  8.80MB/s    in 1.8s    \n",
            "\n",
            "2023-05-19 02:28:27 (8.80 MB/s) - â€˜07_efficientnetb0_feature_extract_model_mixed_precision.zipâ€™ saved [16976857/16976857]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the saved model from Google Storage\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b09f43f-b0cd-4300-8605-67ecde8e5181",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b09f43f-b0cd-4300-8605-67ecde8e5181",
        "outputId": "a84930d6-4144-42d4-d87d-4e9654ffa278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  07_efficientnetb0_feature_extract_model_mixed_precision.zip\n",
            "   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/\n",
            "   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/\n",
            "  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.data-00000-of-00001  \n",
            "  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.index  \n",
            "  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/saved_model.pb  \n",
            "   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/assets/\n"
          ]
        }
      ],
      "source": [
        "# Unzip the SavedModel downloaded from Google Stroage\n",
        "!mkdir downloaded_gs_model # create new dir to store downloaded feature extraction model\n",
        "!unzip 07_efficientnetb0_feature_extract_model_mixed_precision.zip -d downloaded_gs_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "402dc7fb-2ea5-4b5e-8c13-08e5589f8940",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "402dc7fb-2ea5-4b5e-8c13-08e5589f8940",
        "outputId": "f2f25cf8-bb2e-47e1-8085-558e8482da1a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e2f0fc9a5c0b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and evaluate downloaded GS model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_gs_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"07_efficientnetb0_feature_extract_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# Load and evaluate downloaded GS model\n",
        "loaded_gs_model = tf.keras.models.load_model(\"07_efficientnetb0_feature_extract_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1978476b-9d4f-4b87-8448-4cebad39c51a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1978476b-9d4f-4b87-8448-4cebad39c51a",
        "outputId": "62e97e95-8065-4ac6-f3df-f57ba3ccce70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n",
            " )                           )                                   \n",
            "                                                                 \n",
            " pooling_layer (GlobalAvera  (None, 1280)              0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               129381    \n",
            "                                                                 \n",
            " softmax_float32 (Activatio  (None, 101)               0         \n",
            " n)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4178952 (15.94 MB)\n",
            "Trainable params: 129381 (505.39 KB)\n",
            "Non-trainable params: 4049571 (15.45 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Get a summary of our downloaded model\n",
        "loaded_gs_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59acbfcd-8bb0-4528-8265-71400cf488af",
      "metadata": {
        "id": "59acbfcd-8bb0-4528-8265-71400cf488af"
      },
      "source": [
        "And now let's make sure our loaded model is performing as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb87cb50-36d1-408d-b606-83ebab34fa9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb87cb50-36d1-408d-b606-83ebab34fa9c",
        "outputId": "2c63b688-c083-407a-d8d7-1236b0e69680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "790/790 [==============================] - 15s 16ms/step - loss: 1.0881 - accuracy: 0.7067\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.0880972146987915, 0.7066534757614136]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How does the loaded model perform?\n",
        "results_loaded_gs_model = loaded_gs_model.evaluate(test_data)\n",
        "results_loaded_gs_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27177391-7647-46e9-bfa6-f5ebd934d524",
      "metadata": {
        "id": "27177391-7647-46e9-bfa6-f5ebd934d524"
      },
      "source": [
        "Great, our loaded model is performing as expected.\n",
        "\n",
        "When we first created our model, we froze all of the layers in the base model by setting `base_model.trainable=False` but since we've loaded in our model from file, let's check whether or not the layers are trainable or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf5be86-4539-4e19-aef6-1ae66d50f2cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cf5be86-4539-4e19-aef6-1ae66d50f2cb",
        "outputId": "f1deb2c6-5547-4bd3-8862-3a99e2040be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_layer True float32 <Policy \"float32\">\n",
            "efficientnetb0 True float32 <Policy \"mixed_float16\">\n",
            "pooling_layer True float32 <Policy \"mixed_float16\">\n",
            "dense True float32 <Policy \"mixed_float16\">\n",
            "softmax_float32 True float32 <Policy \"float32\">\n"
          ]
        }
      ],
      "source": [
        "# Are any of the layers in our model frozen?\n",
        "for layer in loaded_gs_model.layers:\n",
        "    layer.trainable = True # set all layers to trainable\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af719483-1dbc-44ba-91a1-9cf0b69c52ca",
      "metadata": {
        "id": "af719483-1dbc-44ba-91a1-9cf0b69c52ca"
      },
      "source": [
        "Alright, it seems like each layer in our loaded model is trainable. But what if we got a little deeper and inspected each of the layers in our base model?\n",
        "\n",
        "> ðŸ¤” **Question:** *Which layer in the loaded model is our base model?*\n",
        "\n",
        "Before saving the Functional model to file, we created it with five layers (layers below are 0-indexed):\n",
        "0. The input layer\n",
        "1. The pre-trained base model layer ([`tf.keras.applications.efficientnet.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/EfficientNetB0))\n",
        "2. The pooling layer\n",
        "3. The fully-connected (dense) layer\n",
        "4. The output softmax activation (with float32 dtype)\n",
        "\n",
        "Therefore to inspect our base model layer, we can access the `layers` attribute of the layer at index 1 in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c767b7fb-36da-401d-afcb-c701b748e16c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c767b7fb-36da-401d-afcb-c701b748e16c",
        "outputId": "e2ed90c9-8333-4fdf-f59a-0306192ee914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_1 True float32 <Policy \"float32\">\n",
            "rescaling True float32 <Policy \"mixed_float16\">\n",
            "normalization True float32 <Policy \"float32\">\n",
            "stem_conv_pad True float32 <Policy \"mixed_float16\">\n",
            "stem_conv True float32 <Policy \"mixed_float16\">\n",
            "stem_bn True float32 <Policy \"mixed_float16\">\n",
            "stem_activation True float32 <Policy \"mixed_float16\">\n",
            "block1a_dwconv True float32 <Policy \"mixed_float16\">\n",
            "block1a_bn True float32 <Policy \"mixed_float16\">\n",
            "block1a_activation True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_squeeze True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reshape True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reduce True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_expand True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_excite True float32 <Policy \"mixed_float16\">\n",
            "block1a_project_conv True float32 <Policy \"mixed_float16\">\n",
            "block1a_project_bn True float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_conv True float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_bn True float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_activation True float32 <Policy \"mixed_float16\">\n"
          ]
        }
      ],
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in loaded_gs_model.layers[1].layers[:20]:\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a41092a-c1fc-4aad-aa95-5f4ec45346c2",
      "metadata": {
        "id": "9a41092a-c1fc-4aad-aa95-5f4ec45346c2"
      },
      "source": [
        "Wonderful, it looks like each layer in our base model is trainable (unfrozen) and every layer which should be using the dtype policy `\"mixed_policy16\"` is using it.\n",
        "\n",
        "Since we've got so much data (750 images x 101 training classes = 75750 training images), let's keep all of our base model's layers unfrozen.\n",
        "\n",
        "> ðŸ”‘ **Note:** If you've got a small amount of data (less than 100 images per class), you may want to only unfreeze and fine-tune a small number of layers in the base model at a time. Otherwise, you risk overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caef02fc-bd6a-4dd5-85b9-b28a3b20aede",
      "metadata": {
        "id": "caef02fc-bd6a-4dd5-85b9-b28a3b20aede"
      },
      "source": [
        "## A couple more callbacks\n",
        "\n",
        "We're about to start fine-tuning a deep learning model with over 200 layers using over 100,000 (75k+ training, 25K+ testing) images, which means our model's training time is probably going to be much longer than before.\n",
        "\n",
        "> ðŸ¤” **Question:** *How long does training take?*\n",
        "\n",
        "It could be a couple of hours or in the case of the [DeepFood paper](https://arxiv.org/pdf/1606.05675.pdf) (the baseline we're trying to beat), their best performing model took 2-3 days of training time.\n",
        "\n",
        "You will really only know how long it'll take once you start training.\n",
        "\n",
        "> ðŸ¤” **Question:** *When do you stop training?*\n",
        "\n",
        "Ideally, when your model stops improving. But again, due to the nature of deep learning, it can be hard to know when exactly a model will stop improving.\n",
        "\n",
        "Luckily, there's a solution: the [`EarlyStopping` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping).\n",
        "\n",
        "The `EarlyStopping` callback monitors a specified model performance metric (e.g. `val_loss`) and when it stops improving for a specified number of epochs, automatically stops training.\n",
        "\n",
        "Using the `EarlyStopping` callback combined with the `ModelCheckpoint` callback saving the best performing model automatically, we could keep our model training for an unlimited number of epochs until it stops improving.\n",
        "\n",
        "Let's set both of these up to monitor our model's `val_loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2668c2-2406-46a5-a325-331234c64335",
      "metadata": {
        "id": "bc2668c2-2406-46a5-a325-331234c64335"
      },
      "outputs": [],
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0d3270a-393d-450d-9d8c-90abf2667738",
      "metadata": {
        "id": "f0d3270a-393d-450d-9d8c-90abf2667738"
      },
      "source": [
        "Woohoo! Fine-tuning callbacks ready.\n",
        "\n",
        "If you're planning on training large models, the `ModelCheckpoint` and `EarlyStopping` are two callbacks you'll want to become very familiar with.\n",
        "\n",
        "We're almost ready to start fine-tuning our model but there's one more callback we're going to implement: [`ReduceLROnPlateau`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau).\n",
        "\n",
        "Remember how the learning rate is the most important model hyperparameter you can tune? (if not, treat this as a reminder).\n",
        "\n",
        "Well, the `ReduceLROnPlateau` callback helps to tune the learning rate for you.\n",
        "\n",
        "Like the `ModelCheckpoint` and `EarlyStopping` callbacks, the `ReduceLROnPlateau` callback montiors a specified metric and when that metric stops improving, it reduces the learning rate by a specified factor (e.g. divides the learning rate by 10).\n",
        "\n",
        "> ðŸ¤” **Question:** *Why lower the learning rate?*\n",
        "\n",
        "Imagine having a coin at the back of the couch and you're trying to grab with your fingers.\n",
        "\n",
        "Now think of the learning rate as the size of the movements your hand makes towards the coin.\n",
        "\n",
        "The closer you get, the smaller you want your hand movements to be, otherwise the coin will be lost.\n",
        "\n",
        "Our model's ideal performance is the equivalent of grabbing the coin. So as training goes on and our model gets closer and closer to it's ideal performance (also called **convergence**), we want the amount it learns to be less and less.\n",
        "\n",
        "To do this we'll create an instance of the `ReduceLROnPlateau` callback to monitor the validation loss just like the `EarlyStopping` callback.\n",
        "\n",
        "Once the validation loss stops improving for two or more epochs, we'll reduce the learning rate by a factor of 5 (e.g. `0.001` to `0.0002`).\n",
        "\n",
        "And to make sure the learning rate doesn't get too low (and potentially result in our model learning nothing), we'll set the minimum learning rate to `1e-7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2d24c1-7909-43da-bb27-5e6035806bb4",
      "metadata": {
        "id": "cb2d24c1-7909-43da-bb27-5e6035806bb4"
      },
      "outputs": [],
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f03fb9-b8b2-403b-b4e8-2a7934631a1e",
      "metadata": {
        "id": "53f03fb9-b8b2-403b-b4e8-2a7934631a1e"
      },
      "source": [
        "Learning rate reduction ready to go!\n",
        "\n",
        "Now before we start training, we've got to recompile our model.\n",
        "\n",
        "We'll use sparse categorical crossentropy as the loss and since we're fine-tuning, we'll use a 10x lower learning rate than the Adam optimizers default (`1e-4` instead of `1e-3`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fffadf-49f9-447c-ac44-ad59f5e07e25",
      "metadata": {
        "id": "e8fffadf-49f9-447c-ac44-ad59f5e07e25"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "loaded_gs_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
        "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
        "                        metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd557de-314b-4627-a0fd-f50e0bdb7ee7",
      "metadata": {
        "id": "7fd557de-314b-4627-a0fd-f50e0bdb7ee7"
      },
      "source": [
        "Okay, model compiled.\n",
        "\n",
        "Now let's fit it on all of the data.\n",
        "\n",
        "We'll set it up to run for up to 100 epochs.\n",
        "\n",
        "Since we're going to be using the `EarlyStopping` callback, it might stop before reaching 100 epochs.\n",
        "\n",
        "> ðŸ”‘ **Note:** Running the cell below will set the model up to fine-tune all of the pre-trained weights in the base model on all of the Food101 data. Doing so with **unoptimized** data pipelines and **without** mixed precision training will take a fairly long time per epoch depending on what type of GPU you're using (about 15-20 minutes on Colab GPUs). But don't worry, **the code we've written above will ensure it runs much faster** (more like 4-5 minutes per epoch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514dc1e0-0899-4831-9442-d007a8f4782b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514dc1e0-0899-4831-9442-d007a8f4782b",
        "outputId": "2a20ee39-f2d2-4594-ba5c-14d22b60555f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20230519-022854\n",
            "Epoch 1/100\n",
            "2368/2368 [==============================] - 246s 81ms/step - loss: 0.9223 - accuracy: 0.7525 - val_loss: 0.7872 - val_accuracy: 0.7749 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "2368/2368 [==============================] - 191s 81ms/step - loss: 0.5795 - accuracy: 0.8399 - val_loss: 0.7839 - val_accuracy: 0.7831 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "2368/2368 [==============================] - 162s 68ms/step - loss: 0.3299 - accuracy: 0.9063 - val_loss: 0.8827 - val_accuracy: 0.7765 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9486\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "2368/2368 [==============================] - 162s 68ms/step - loss: 0.1722 - accuracy: 0.9486 - val_loss: 0.9571 - val_accuracy: 0.7850 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "2368/2368 [==============================] - 162s 68ms/step - loss: 0.0359 - accuracy: 0.9920 - val_loss: 1.0549 - val_accuracy: 0.8032 - lr: 2.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# Start to fine-tune (all layers)\n",
        "history_101_food_classes_all_data_fine_tune = loaded_gs_model.fit(train_data,\n",
        "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
        "                                                        steps_per_epoch=len(train_data),\n",
        "                                                        validation_data=test_data,\n",
        "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
        "                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
        "                                                                   model_checkpoint, # save only the best model during training\n",
        "                                                                   early_stopping, # stop model after X epochs of no improvements\n",
        "                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d58b2ebd-1955-416c-a76d-ec0999a25736",
      "metadata": {
        "id": "d58b2ebd-1955-416c-a76d-ec0999a25736"
      },
      "source": [
        "> ðŸ”‘ **Note:** If you didn't use mixed precision or use techniques such as [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) in the *Batch & prepare datasets* section, your model fine-tuning probably takes up to 2.5-3x longer per epoch (see the output below for an example).\n",
        "\n",
        "| | Prefetch and mixed precision | No prefetch and no mixed precision |\n",
        "|-----|-----|-----|\n",
        "| Time per epoch | ~280-300s | ~1127-1397s |\n",
        "\n",
        "*Results from fine-tuning ðŸ”ðŸ‘ Food Vision Bigâ„¢ on Food101 dataset using an EfficienetNetB0 backbone using a Google Colab Tesla T4 GPU.*\n",
        "\n",
        "```\n",
        "Saving TensorBoard log files to: training_logs/efficientB0_101_classes_all_data_fine_tuning/20200928-013008\n",
        "Epoch 1/100\n",
        "2368/2368 [==============================] - 1397s 590ms/step - loss: 1.2068 - accuracy: 0.6820 - val_loss: 1.1623 - val_accuracy: 0.6894\n",
        "Epoch 2/100\n",
        "2368/2368 [==============================] - 1193s 504ms/step - loss: 0.9459 - accuracy: 0.7444 - val_loss: 1.1549 - val_accuracy: 0.6872\n",
        "Epoch 3/100\n",
        "2368/2368 [==============================] - 1143s 482ms/step - loss: 0.7848 - accuracy: 0.7838 - val_loss: 1.0402 - val_accuracy: 0.7142\n",
        "Epoch 4/100\n",
        "2368/2368 [==============================] - 1127s 476ms/step - loss: 0.6599 - accuracy: 0.8149 - val_loss: 0.9599 - val_accuracy: 0.7373\n",
        "```\n",
        "*Example fine-tuning time for non-prefetched data as well as non-mixed precision training (~2.5-3x longer per epoch).*\n",
        "\n",
        "Let's make sure we save our model before we start evaluating it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ce480d-7f0a-40fc-b42e-3d28cfadd462",
      "metadata": {
        "id": "00ce480d-7f0a-40fc-b42e-3d28cfadd462"
      },
      "source": [
        "From the above, does it look like our model is overfitting or underfitting?\n",
        "\n",
        "Remember, if the training loss is significantly lower than the validation loss, it's a hint that the model has overfit the training data and not learned generalizable patterns to unseen data.\n",
        "\n",
        "But it does look like our model has gained a few performance points from fine-tuning, let's evaluate on the whole test dataset and see if managed to beat the [DeepFood paper's](https://arxiv.org/abs/1606.05675) result of 77.4% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483515ad-3ae8-4bf9-b484-fef56b87ed4c",
      "metadata": {
        "id": "483515ad-3ae8-4bf9-b484-fef56b87ed4c"
      },
      "outputs": [],
      "source": [
        "# # Save model to Google Drive (optional)\n",
        "# loaded_gs_model.save(\"/content/drive/MyDrive/tensorflow_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578aea68-5351-4fea-a4b4-eeedb511c71b",
      "metadata": {
        "id": "578aea68-5351-4fea-a4b4-eeedb511c71b"
      },
      "outputs": [],
      "source": [
        "# Save model locally (note: if you're using Google Colab and you save your model locally, it will be deleted when your Google Colab session ends)\n",
        "loaded_gs_model.save(\"07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312cb4c9-e399-4efb-8258-828a3f9291f9",
      "metadata": {
        "id": "312cb4c9-e399-4efb-8258-828a3f9291f9"
      },
      "source": [
        "## Download fine-tuned model from Google Storage\n",
        "\n",
        "As mentioned before, training models can take a significant amount of time.\n",
        "\n",
        "And again, like any good cooking show, here's something we prepared earlier...\n",
        "\n",
        "It's a fine-tuned model exactly like the one we trained above but it's saved to Google Storage so it can be accessed, imported and evaluated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325e55b2-dee4-4e3f-9113-82c9e749efbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "325e55b2-dee4-4e3f-9113-82c9e749efbb",
        "outputId": "4484f0f3-1127-4246-89e8-a9f8f242421c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-19 02:44:48--  https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 142.251.10.128, 142.251.12.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46790356 (45M) [application/zip]\n",
            "Saving to: â€˜07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zipâ€™\n",
            "\n",
            "07_efficientnetb0_f 100%[===================>]  44.62M  14.1MB/s    in 3.2s    \n",
            "\n",
            "2023-05-19 02:44:51 (14.1 MB/s) - â€˜07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zipâ€™ saved [46790356/46790356]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and evaluate fine-tuned model from Google Storage\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5f856e-e58f-4575-b725-ca7b5004af10",
      "metadata": {
        "id": "1c5f856e-e58f-4575-b725-ca7b5004af10"
      },
      "source": [
        "The downloaded model comes in zip format (`.zip`) so we'll unzip it into the Google Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b9f6ba-1d0b-4043-a55d-40bef03d816b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29b9f6ba-1d0b-4043-a55d-40bef03d816b",
        "outputId": "6e5a516a-7f60-4783-e3ce-06f0dd649592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\n",
            "   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/\n",
            "   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/\n",
            "  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.data-00000-of-00001  \n",
            "  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.index  \n",
            "  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/saved_model.pb  \n",
            "   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/assets/\n"
          ]
        }
      ],
      "source": [
        "# Unzip fine-tuned model\n",
        "!mkdir downloaded_fine_tuned_gs_model # create separate directory for fine-tuned model downloaded from Google Storage\n",
        "!unzip 07_efficientnetb0_fine_tuned_101_classes_mixed_precision -d downloaded_fine_tuned_gs_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584eb626-33ea-489e-be8a-ba8517035688",
      "metadata": {
        "id": "584eb626-33ea-489e-be8a-ba8517035688"
      },
      "source": [
        "Now we can load it using the [`tf.keras.models.load_model()`](https://www.tensorflow.org/tutorials/keras/save_and_load) method and get a summary (it should be the exact same as the model we created above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0f85d6-3613-48bf-8ce1-fb6b58865545",
      "metadata": {
        "id": "1c0f85d6-3613-48bf-8ce1-fb6b58865545"
      },
      "outputs": [],
      "source": [
        "# Load in fine-tuned model from Google Storage and evaluate\n",
        "loaded_fine_tuned_gs_model = tf.keras.models.load_model(\"downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861798eb-0a34-4d91-ae7a-0b0b78561fdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861798eb-0a34-4d91-ae7a-0b0b78561fdf",
        "outputId": "55b58742-53e8-4265-e7c1-2386350df88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n",
            " )                           )                                   \n",
            "                                                                 \n",
            " pooling_layer (GlobalAvera  (None, 1280)              0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               129381    \n",
            "                                                                 \n",
            " softmax_float32 (Activatio  (None, 101)               0         \n",
            " n)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4178952 (15.94 MB)\n",
            "Trainable params: 4136929 (15.78 MB)\n",
            "Non-trainable params: 42023 (164.16 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Get a model summary (same model architecture as above)\n",
        "loaded_fine_tuned_gs_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235fa9a8-ba2e-484e-aad2-dc2c00855934",
      "metadata": {
        "id": "235fa9a8-ba2e-484e-aad2-dc2c00855934"
      },
      "source": [
        "Finally, we can evaluate our model on the test data (this requires the `test_data` variable to be loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2276cadc-c4b2-4e9a-b7b6-15746ea6eb20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2276cadc-c4b2-4e9a-b7b6-15746ea6eb20",
        "outputId": "2ea451e1-21a8-465c-e75d-a8b576fe70ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "790/790 [==============================] - 15s 16ms/step - loss: 0.9072 - accuracy: 0.8017\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.9072489738464355, 0.801663339138031]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Note: Even if you're loading in the model from Google Storage, you will still need to load the test_data variable for this cell to work\n",
        "results_downloaded_fine_tuned_gs_model = loaded_fine_tuned_gs_model.evaluate(test_data)\n",
        "results_downloaded_fine_tuned_gs_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92419d08-3803-43bd-b588-9359e4a720d9",
      "metadata": {
        "id": "92419d08-3803-43bd-b588-9359e4a720d9"
      },
      "source": [
        "Excellent! Our saved model is performing as expected (better results than the DeepFood paper!).\n",
        "\n",
        "Congrautlations! You should be excited! You just trained a computer vision model with competitive performance to a research paper and in far less time (our model took ~20 minutes to train versus DeepFood's quoted 2-3 days).\n",
        "\n",
        "In other words, you brought Food Vision life!\n",
        "\n",
        "If you really wanted to step things up, you could try using the [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) model (a larger version of `EfficientNetB0`). At at the time of writing, the EfficientNet family has the [state of the art classification results](https://paperswithcode.com/sota/fine-grained-image-classification-on-food-101) on the Food101 dataset.\n",
        "\n",
        "> ðŸ“– **Resource:** To see which models are currently performing the best on a given dataset or problem type as well as the latest trending machine learning research, be sure to check out [paperswithcode.com](http://paperswithcode.com/) and [sotabench.com](https://sotabench.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b6846d-f542-47b8-979b-65215bd12a74",
      "metadata": {
        "id": "24b6846d-f542-47b8-979b-65215bd12a74"
      },
      "source": [
        "## View training results on TensorBoard\n",
        "\n",
        "Since we tracked our model's fine-tuning training logs using the `TensorBoard` callback, let's upload them and inspect them on TensorBoard.dev."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf8e3fb-1169-4160-8486-9f4a50a1ab4e",
      "metadata": {
        "id": "8bf8e3fb-1169-4160-8486-9f4a50a1ab4e"
      },
      "outputs": [],
      "source": [
        "# Upload experiment results to TensorBoard (uncomment to run)\n",
        "# !tensorboard dev upload --logdir ./training_logs \\\n",
        "#   --name \"Fine-tuning EfficientNetB0 on all Food101 Data\" \\\n",
        "#   --description \"Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001\" \\\n",
        "#   --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c9e3985-0626-456c-9d45-e5b3134eb5e5",
      "metadata": {
        "id": "3c9e3985-0626-456c-9d45-e5b3134eb5e5"
      },
      "source": [
        "Viewing at our [model's training curves on TensorBoard.dev](https://tensorboard.dev/experiment/2KINdYxgSgW2bUg7dIvevw/), it looks like our fine-tuning model gains boost in performance but starts to overfit as training goes on.\n",
        "\n",
        "See the training curves on TensorBoard.dev here: https://tensorboard.dev/experiment/2KINdYxgSgW2bUg7dIvevw/\n",
        "\n",
        "To fix this, in future experiments, we might try things like:\n",
        "* A different iteration of `EfficientNet` (e.g. `EfficientNetB4` instead of `EfficientNetB0`).\n",
        "* Unfreezing less layers of the base model and training them rather than unfreezing the whole base model in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5052bf8-29d4-48c0-8314-3c9ab6cb6aef",
      "metadata": {
        "id": "d5052bf8-29d4-48c0-8314-3c9ab6cb6aef"
      },
      "outputs": [],
      "source": [
        "# View past TensorBoard experiments\n",
        "# !tensorboard dev list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f132dd1e-653f-46b6-9adb-e084bac84a84",
      "metadata": {
        "id": "f132dd1e-653f-46b6-9adb-e084bac84a84"
      },
      "outputs": [],
      "source": [
        "# Delete past TensorBoard experiments\n",
        "# !tensorboard dev delete --experiment_id YOUR_EXPERIMENT_ID\n",
        "\n",
        "# Example\n",
        "# !tensorboard dev delete --experiment_id OAE6KXizQZKQxDiqI3cnUQ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86fa758-b1bf-4113-88f7-c3d2996b93f5",
      "metadata": {
        "id": "d86fa758-b1bf-4113-88f7-c3d2996b93f5"
      },
      "source": [
        "## ðŸ›  Exercises\n",
        "\n",
        "1. Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook ([Transfer Learning Part 3: Scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)). More specifically, it would be good to see:\n",
        "  * A confusion matrix between all of the model's predictions and true labels.\n",
        "  * A graph showing the f1-scores of each class.\n",
        "  * A visualization of the model making predictions on various images and comparing the predictions to the ground truth.\n",
        "    * For example, plot a sample image from the test dataset and have the title of the plot show the prediction, the prediction probability and the ground truth label.\n",
        "2. Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students.\n",
        "3. Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) as the base model instead of `EfficientNetB0`. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?\n",
        "4. Name one important benefit of mixed precision training, how does this benefit take place?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78edd4a-9363-4816-a828-36d9c6ff23d4",
      "metadata": {
        "id": "f78edd4a-9363-4816-a828-36d9c6ff23d4"
      },
      "source": [
        "## ðŸ“– Extra-curriculum\n",
        "\n",
        "* Read up on learning rate scheduling and the [learning rate scheduler callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler). What is it? And how might it be helpful to this project?\n",
        "* Read up on TensorFlow data loaders ([improving TensorFlow data loading performance](https://www.tensorflow.org/guide/data_performance)). Is there anything we've missed? What methods you keep in mind whenever loading data in TensorFlow? Hint: check the summary at the bottom of the page for a gret round up of ideas.\n",
        "* Read up on the documentation for [TensorFlow mixed precision training](https://www.tensorflow.org/guide/mixed_precision). What are the important things to keep in mind when using mixed precision training?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06eb0ec7e76c4e13a52c6fe64176cbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd0039e3e104a9799ce92f830de6b87",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4388427f02b745ac9ceadd70a2bf3f71",
            "value": 0
          }
        },
        "09d4a35af4e94860bcc45482bfd11c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "106d43e34bd449a2ba467951ec262b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a338dc88995461c82e3088daa647f32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd0039e3e104a9799ce92f830de6b87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3070680f2783400790aeff5475a559f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30d307e20e19487eb77bfacb9e5db6cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf45f4fb71f14761bd497740051c7e24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09d4a35af4e94860bcc45482bfd11c36",
            "value": 1
          }
        },
        "379aed682ef94d73adaaea88ca54278a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e5ccd7b921646ce9c98c63650c0b91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a338dc88995461c82e3088daa647f32",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f32318cbbd5444f29ab875a2a63246b1",
            "value": "Dlâ€‡Completed...:â€‡â€‡â€‡0%"
          }
        },
        "4388427f02b745ac9ceadd70a2bf3f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "498125d566fc4b479977d2fcb670686b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb8b9828f3a49abab40e64cd09af9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_636f6915f8414efcaade38959ff052ef",
              "IPY_MODEL_30d307e20e19487eb77bfacb9e5db6cb",
              "IPY_MODEL_f42698984d274febbb077cd115cd41ca"
            ],
            "layout": "IPY_MODEL_cc04768d14c94f7db02e686414425576"
          }
        },
        "53de0d0d65ba48fca265882843128ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e6d6f2317b49a080f3fb5d8d2b1936",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_96aa21c821a3440fb2e20a80a34e5101",
            "value": "â€‡0/1â€‡[01:53&lt;?,â€‡?â€‡url/s]"
          }
        },
        "5a8d50941efd48b9abb954658469efc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c37537333345c58b911cbd47f65a12",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ddbe04d22b3480fa898aec7b9b0ca17",
            "value": 0
          }
        },
        "60259aa3d1d14c94a34fd2592244ce8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636f6915f8414efcaade38959ff052ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe9583bfed6c498a8a22df36567ed585",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_379aed682ef94d73adaaea88ca54278a",
            "value": "Dlâ€‡Size...:â€‡â€‡60%"
          }
        },
        "6960acc878f842ea827fcd7a3bba8034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e5ccd7b921646ce9c98c63650c0b91c",
              "IPY_MODEL_5a8d50941efd48b9abb954658469efc2",
              "IPY_MODEL_53de0d0d65ba48fca265882843128ab2"
            ],
            "layout": "IPY_MODEL_498125d566fc4b479977d2fcb670686b"
          }
        },
        "6ddbe04d22b3480fa898aec7b9b0ca17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bd2692da1194d56acf1a497a0e81afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab73b901ce74498a839d328834fdab0c",
              "IPY_MODEL_06eb0ec7e76c4e13a52c6fe64176cbf5",
              "IPY_MODEL_f076394f8e624bbf8a0550be45923395"
            ],
            "layout": "IPY_MODEL_3070680f2783400790aeff5475a559f5"
          }
        },
        "83ca4a1705504b8a9c1b9f10973409e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96aa21c821a3440fb2e20a80a34e5101": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4c37537333345c58b911cbd47f65a12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a51ec4caffae40b5aa0c6090c57c2050": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab73b901ce74498a839d328834fdab0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60259aa3d1d14c94a34fd2592244ce8a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_106d43e34bd449a2ba467951ec262b06",
            "value": "Extractionâ€‡completed...:â€‡"
          }
        },
        "bf45f4fb71f14761bd497740051c7e24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c0e6d6f2317b49a080f3fb5d8d2b1936": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb84865f4e4b4b8590ef0a053c5eccce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc04768d14c94f7db02e686414425576": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db683af2888a4ae7a07b4e472b9ea617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f076394f8e624bbf8a0550be45923395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ca4a1705504b8a9c1b9f10973409e9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db683af2888a4ae7a07b4e472b9ea617",
            "value": "â€‡0/0â€‡[01:53&lt;?,â€‡?â€‡file/s]"
          }
        },
        "f32318cbbd5444f29ab875a2a63246b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f42698984d274febbb077cd115cd41ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51ec4caffae40b5aa0c6090c57c2050",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cb84865f4e4b4b8590ef0a053c5eccce",
            "value": "â€‡2870/4764â€‡[01:53&lt;01:10,â€‡26.93â€‡MiB/s]"
          }
        },
        "fe9583bfed6c498a8a22df36567ed585": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
